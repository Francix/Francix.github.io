
![](https://francix.github.io/images/ESB.jpg)

### Bayesian's Creed

Never let your shallow understanding override the truth of the world.

But, fight for your deepest belief no mater what the world is. 

### BIO

Yao Francis Fu угдт░Д 

I am an MSCS student at Columbia University. I will be coming to University of Edinburgh in Spring 20 as a Ph.D. student

Generally, I am interested in the interaction between the machine and the world through Human Language. 
Natural Language Understanding is an essential step to achieve this goal, particularly and practically at sentence level. 
This includes how the machine shall learn the world through knowledge reprensentation, how the machine may understand the world through language understanding, and how to perform meaningful interaction through language generation. 

In terms of specific topics, I am interested in 
* [Text Generation](https://francix.github.io/blog/yaofu_NLG.pdf), Knowledge Representation Learning
* Bayesian Probablistic Modeling, Deep Generative Models
* Causality in Text, Model Interpretability 

I am also maintaining the reading lists about:
* Deep Generative Models for Natural Language Processing. ([link](https://github.com/Francix/Deep-Generative-Models-for-Natural-Language-Processing))
* Language Model Pretraining for Text Generation ([link](https://github.com/Francix/Language-Model-Pretraining-for-Text-Generation))
* In my opinion, the two most compelling problems of NLG are (1) decoding and (2) evaluation. I am consistently focusing on this. 

During my MS study at Columbia, between ML and NLP, I focus more on the ML side. Recently I am particularly interested in [Deep Generative Models](http://stat.columbia.edu/~cunningham/teaching/GR8201/) and [Causal Inference for NLP](https://arxiv.org/abs/1802.02163). I will switch back to the NLP side in my Ph.D. study at Edinburgh. 

Beside research, I love photography in my spare time. Follow [my ins](https://www.instagram.com/franx_yao/)! 

-----

### UPDATES

* Apr 29 2019. I gave a talk about [table to text generation and evaluation](https://francix.github.io/blog/on_text_generation_evaluation) at [Interactions](https://www.interactions.com/). 

* Apr 09 2019. I am maintaining a reading list about [Deep Generative Models for Natural Language Processing](https://github.com/Francix/Deep-Generative-Models-for-Natural-Language-Processing). This is my reading list in the Columbia DGM seminar class, and I gave a detailed presentation about the [Adversarially Regularizaed Autoencoder paper](https://francix.github.io/blog/annotated_arae.pdf). 

* Sep 15 2018. I did an Emotional Speech Analysis project at the Columbia Speech Lab, under supervision from professor [Julia Hirschberg](http://www.cs.columbia.edu/~julia/). 

  * See the [blog about this project](https://Francix.github.io/blog/Attempts-on-the-Emotional-Speech-Generation).

* Feb 15 2018, Our paper [Natural Answer Generation with Heterogeneous Memory](https://francix.github.io/NaturalAnswerGeneration.pdf) is accepted by NAACL-HLT 2018!  

  * This is [a QA system aims to compose answer words into sentences](https://francix.github.io/NaturalAnswer.html). Given a question, the system is able to read the question, find answer words from information of different sources, compose these words into sentences with related information in the memory.
  * This work is my summer research project under supervision of Professor [Yansong Feng](https://sites.google.com/site/ysfeng/home)

* Feb 05 2018, I gave an [introduction](https://francix.github.io/MemNN-Fuyao-EN.html)([CN](https://francix.github.io/MemNN-Fuyao-CN.html)) ([EN pdf](https://francix.github.io/MemNN-Fuyao-EN.pdf), [CN pdf](https://francix.github.io/MemNN-Fuyao-CN.pdf)) about Memory Networks at Bytedance AI Lab. 

  - My summer research in 2017 is closely related to the memory networks. In this tutorial, I discussed multiple variants of attention mechanisms, and clarified the differences between matching based machine reading and reasoning based machine reading. 

-----

### PROJECTS
* Paraphrase Generation with Latent Bag of Words. (Current)

  * [Columbia Stats Deep Generative Models seminar](http://stat.columbia.edu/~cunningham/teaching/GR8201/), advised by prof. John Cunningham. course project. 

* Assessing the BERT Language Model pretraining influence on text generation tasks. (in submission). Dec 18 - 

  * The recent BERT model use a Maskded-LM Pretraining method to improve the classification/ structural prediction performance significantly, how will it influence text generation tasks? 
  * We study the BERT pre-training methods, as well as other pre-training methods for text generation. 
  * We find out that the BERT-style Masked LM CANNOT really increase the generation performance, but the left-to-right LM can. 
  * In extreme scenarios where we only have 1K labeled instances, all models withou pre-training failed (2+ BLEU) because such small amount of data simply cannot support a neural generation model. However, if we initialize the model with a pre-trained LM, the model can still perform relatively well (17+ BLEU)

* Text Style Transfer In-depth (current). Mar 18 - Dec 18 

  * Text style transfer is recently a hot topic in NLP, but few have examined what determines the style and what is changed during the transfer.
  * We analyze the underlying assumptions of state-of-the-art text style transfer systems and their performance. 
  * We study the one-to-many transfer problem and show how it brings new challenges to the existing binary transfer. 
  
* Natural Answer Generation with Heterogeneous Memory. Jul 17 - Dec 17 

  * A QA system aims to compose answer words into sentences with information enrichment. 
  * Publication: Yao Fu and Yansong Feng, _Natural Answer Generation with Heterogeneous Memory_, to appear in NAACL-HLT 2018. 

* [Scheduling and routing models for food rescue and delivery operations](https://github.com/Francix/Multi-Vehicle-Multi-Peroid-Dynamic-Tabu-Search/tree/master). Mar 16 - Dec 16

  * This project was finished during my exchange in University of New South Wales, Australia. 
  * It was a scheduling and routing model for food rescue (i.e. deliver foods to people in need) traffic distribution networks. 
  * It was an extension of my mentor, [Dr. Dyvia Nair](http://www.rciti.unsw.edu.au/staff/divya-nair)'s work in 2016. This work won the Tranportation Research Board 2016 Best Paper Award. 
  * Under Dyvia's supervision, I extended the algorithm into a dynamic version. 
  * Now it is being applied to the [OzHarvest project](http://www.ozharvest.org/), Australia's leading food rescue charity. 
  * Publication: D.J. Nair, H. Grzybowska, Y. Fu, V.V. Dixit, _Scheduling and routing models for food rescue and delivery operations_, Socio-Economic Planning Sciences
  
-----

### Teaching 

* [Columbia COMS 4995 Applied Machine Learning](http://www.cs.columbia.edu/~amueller/comsw4995s19/), 19 Spring, Course Assistant. Tought by prof. Andreas Muller. 

-----

### MY PERSONAL BACKGROUND

* I come from a river town in Hunan Province, southwest China. Like the one in Peter Hessler's [_River Town_](http://www.goodreads.com/book/show/94053.River_Town)). 





